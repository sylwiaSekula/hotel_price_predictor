import numpy as np
import os
import pandas as pd
import pickle
from scripts.settings import *
from sklearn.metrics import make_scorer, median_absolute_error, mean_absolute_error, mean_absolute_percentage_error
from scripts.prepare_data.data_preprocessing import preprocess
from scripts.model.train_and_save_model import median_absolute_percentage_error


def fill_missing_with_knn_test(test_data: pd.DataFrame, columns_to_impute: list, knn_imputer: object) -> pd.DataFrame:
    """
    Impute missing values in test dataset using K-Nearest Neighbors (KNN) imputation with predicted values.
    :param test_data: pd.DataFrame, the test dataset with missing values to be imputed.
    :param columns_to_impute: list, the list of column names to impute missing values for.
    :param knn_imputer: object, a trained K-Nearest Neighbors imputer object
    :return: pd.DataFrame, the imputed test dataset.
    """
    test_data[columns_to_impute] = knn_imputer.transform(test_data[columns_to_impute])
    return test_data


def handle_outliers_test(test_data: pd.DataFrame, column_name: str, isolation_forest: object) -> pd.DataFrame:
    """
    Handle outliers in a DataFrame column using Isolation Forest and replace outliers in a specified column of a
    DataFrame with the median value among the non-outliers.
    :param test_data: pd.DataFrame, the test dataset with outliers
    :param column_name: str: the name of the column with outliers to be handled.
    :param isolation_forest: object: a trained Isolation Forest model object
    :return: pd.DataFrame, the test dataset with outliers replaced by the median non-outlier
    value in the specified column.
    """
    column_values_test = test_data[column_name].values.reshape(-1, 1)
    outliers_test = isolation_forest.predict(column_values_test)
    outlier_indices = test_data.index[outliers_test == -1]
    non_outliers = test_data.loc[~test_data.index.isin(outlier_indices), column_name]
    median_non_outlier = non_outliers.median()
    test_data.loc[outlier_indices, column_name] = median_non_outlier
    return test_data


def evaluate_model(y_test: pd.Series, y_pred: pd.Series) -> (float, float, float, float):
    """
    Evaluate the performance of a regression model by computing three different error metrics: Median Absolute Error
    (MedAE), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE) and Median Absolute Percentage Error
    (MdAPE).
    :param y_test: pd.Series, the actual target values (ground truth) for the test dataset.
    :param y_pred: pd.Series, the predicted target values generated by a regression model.
    :return:Tuple(float, float, float, float): a tuple containing the following error metrics: MedAE, MAE, MAPE and
    MdAPE
    """
    median_absolute_err = median_absolute_error(y_test, y_pred)
    mean_absolute_err = mean_absolute_error(y_test, y_pred)
    mape = mean_absolute_percentage_error(y_test, y_pred)
    mdape = median_absolute_percentage_error(y_test, y_pred)
    return median_absolute_err, mean_absolute_err, mape, mdape


def main():
    columns_to_impute = ['rating_score', 'reviews']
    columns_to_encode = ['room_type', 'city']
    target = 'price'
    output_processed_test_path = ('../prepare_data/df_processed_test.csv')
    # load the test dataset
    df_test = pd.read_csv('../prepare_data/df_test.csv')
    # preprocess the test dataset
    df_test = preprocess(df_test)
    # load trained models knn_imputer and isolation forest
    knn_imputer = pickle.load(open(os.path.join(trained_model_dir, knn_imputer_file), 'rb'))
    isolation_forest = pickle.load(open(os.path.join(trained_model_dir, isolation_forest_file), 'rb'))
    # fill missing values on test data
    df_test = fill_missing_with_knn_test(df_test, columns_to_impute, knn_imputer)
    # handle outliers on test data
    df_test = handle_outliers_test(df_test, target, isolation_forest)
    # load the trained ordinal encoder model and encode categorical data
    encoder = pickle.load(open(os.path.join(trained_model_dir, ordinal_encoder_file), 'rb'))
    df_test[columns_to_encode] = encoder.transform(df_test[columns_to_encode])
    # save the processed test dateset for data analysis
    df_test.to_csv(output_processed_test_path, index=False)
    # define X and y
    X_test = df_test.drop(target, axis=1)  # features
    y_test = df_test[target]  # target
    # load the trained scaler and scale test data
    scaler = pickle.load(open(os.path.join(trained_model_dir, scaler_file), 'rb'))
    X_test_scaled = scaler.transform(X_test)

    # load the trained models
    rf_regressor_opt = pickle.load(open(os.path.join(trained_model_dir, random_forest_file), 'rb'))
    xgb_opt = pickle.load(open(os.path.join(trained_model_dir, xgboost_file), 'rb'))
    knn_opt = pickle.load(open(os.path.join(trained_model_dir, knn_file), 'rb'))
    # Predict the target on the test data
    y_pred = rf_regressor_opt.predict(X_test_scaled)
    y_pred_xgb = xgb_opt.predict(X_test_scaled)
    y_pred_knn = knn_opt.predict(X_test_scaled)

    # calculate metrics for each model
    metrics_rf = evaluate_model(y_test, y_pred)
    metrics_xgb = evaluate_model(y_test, y_pred_xgb)
    metrics_knn = evaluate_model(y_test, y_pred_knn)

    #  Print metrics
    print("Random Forest => Metrics:", metrics_rf)
    print("\nXGBoost => Metrics:", metrics_xgb)
    print("\nKNN => Metrics:", metrics_knn)


if __name__ == '__main__':
    main()
